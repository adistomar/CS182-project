wandb_log: false
wandb_project: '182-proj'
wandb_run_name: 'gpt2-mono-alphabetic-sub'

base_out_dir: 'out'

batch_size: 64
block_size: 2048
gradient_accumulation_steps: 1
weight_decay: 1e-1
learning_rate: 1e-3
grad_clip: 1.0 # clip gradients at this value, or disable if == 0.0

decay_lr: False # if True, warm up LR linearly for warmup_iters, then decay down to min lr for lr_decay_iters
warmup_iters: 2000
max_iters: 20000
lr_decay_iters: 20000
min_lr: 1e-4

eval_interval: 100
eval_iters: 50
log_interval: 1
save_interval: 1000
eval_only: False # if True, script exits right after the first eval

dataset: 'openwebtext'

# ddp settings (if applicable)
backend: 'nccl' # 'nccl', 'gloo', etc.

# system
compile: True # use PyTorch 2.0 to compile the model to be faster
scheme_type: 'mono-alphabetic-sub'
